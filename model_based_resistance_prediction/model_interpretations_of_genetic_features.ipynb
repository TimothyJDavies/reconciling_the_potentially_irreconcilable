{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting genetic features into a model readable format\n",
    "\n",
    "\n",
    "\n",
    "**This code takes the genetic features in model_elements_and_coverages.json (produced in the rules based resistance prediction and the converts them into a model readable output which has**\n",
    "\n",
    "1. For each beta-lactamase, which bush jacoby class it is in and what its copy number is (e.g. blaTEM_buj2b=1.23 implies the sample contains a class 2b blaTEM geneat DNA copy number 1.23\n",
    "2. Whether the sample contains mutated ampC promoters of blaTEM promoters (1/0 , 1 if present, 0 otherwise)\n",
    "3. Whether the sample contains \"non-functioning\" ompC/F genes, (1 if present, 0 otherwise)\n",
    "\n",
    "**This is then converted to a useful long format which incorporates the MIC data from **\n",
    "1. bd_phoenix_mics.csv\n",
    "2. agar_dilution_raw_mics.json\n",
    "\n",
    "**Lastly it also adds MLSTs to the data , which are categorised for the 3 most common mlsts in the agar dilution dataset for testing in the model.**\n",
    "\n",
    "#### To run this code you need.\n",
    "- Numpy\n",
    "- Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So the first step is to define beta-lactamase categories\n",
    "\n",
    "Required files\n",
    "1. exclusions.csv => List of 11 exclusions from prediction\n",
    "2. mlsts_plus_ids.csv =>  MLSTS of all samples\n",
    "3. model_elements_and_coverages.json => Model elements and coverage used as the raw genetic features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in data\n",
    "\n",
    "with open(\"model_elements_and_coverages.json\", \"r\") as f:\n",
    "    model_start = json.load(f)\n",
    "genes = [j.split(\".\")[0] for j in list(set([k for g in model_start.keys() for k in model_start[g]['blm'].keys()]))]\n",
    "blms = [i.replace(\"TEM-1D\", 'TEM-1') for i in genes if \"bla\" in i]\n",
    "guuids = list(model_start.keys())\n",
    "mlsts = pd.read_csv(\"mlsts.csv\", index_col=0)\n",
    "agar_guuids = [k for k in mlsts.index if mlsts.loc[k].complete_agardil == 1]\n",
    "exclusions = pd.read_csv(\"exclusions.csv\", index_col=0)\n",
    "\n",
    "\n",
    "# Defining classification categories.\n",
    "\n",
    "#Families\n",
    "TEM = ['blaTEM-1','blaTEM-135', 'blaTEM-206', 'blaTEM-149', \n",
    "       'blaTEM-150', 'blaTEM-156', 'blaTEM-190', 'blaTEM-198',\n",
    "       'blaTEM-207', 'blaTEM-208', 'blaTEM-209', \n",
    "       'blaTEM-30', 'blaTEM-33', 'blaTEM-34', 'blaTEM-71' ]\n",
    "CTXM = ['blaCTX-M-1', 'blaCTX-M-132', 'blaCTX-M-14b', \n",
    "        'blaCTX-M-15', 'blaCTX-M-27', 'blaCTX-M-3',\n",
    "        'blaCTX-M-45', 'blaCTX-M-55', 'blaCTX-M-9']\n",
    "SHV = ['blaSHV-12','blaSHV-11']\n",
    "OXA = ['blaOXA-1','blaOXA-48' ]\n",
    "CMY = ['blaCMY-99','blaCMY-2','blaCMY-60' ]\n",
    "ACT = ['blaACT-16']\n",
    "LAT = ['blaLAT-1']\n",
    "\n",
    "#BUSH Jacoby\n",
    "\n",
    "buj1 = ['blaCMY-2', 'blaACT-16','blaCMY-99', 'blaCMY-60', 'blaLAT-1']\n",
    "buj2b= ['blaTEM-1','blaSHV-11', 'blaTEM-135']\n",
    "buj2be = ['blaCTX-M-15','blaCTX-M-14b', 'blaCTX-M-27',\n",
    "          'blaTEM-209','blaTEM-207' , 'blaTEM-190', 'blaCTX-M-1',\n",
    "          'blaSHV-12', 'blaCTX-M-55', 'blaCTX-M-45','blaCTX-M-9' ,\n",
    "          'blaTEM-156', 'blaTEM-149','blaTEM-198','blaCTX-M-132' ,\n",
    "         'blaCTX-M-3','blaTEM-150','blaTEM-208'  ]\n",
    "buj2br = ['blaTEM-34','blaTEM-30', 'blaTEM-33', 'blaTEM-71', 'blaTEM-206']\n",
    "buj2d = ['blaOXA-1']\n",
    "buj2df = ['blaOXA-48']\n",
    "\n",
    "# Checking these classes\n",
    "bush_jacoby_classes = {\"buj1\":buj1, 'buj2b':buj2b, 'buj2be':buj2be, \n",
    "                       'buj2br':buj2br, 'buj2d':buj2d, 'buj2df':buj2df}\n",
    "bush_jacoby_all = buj1 + buj2b + buj2be + buj2br + buj2d + buj2df\n",
    "families = {\"TEM\":TEM, \"CTXM\":CTXM, \"SHV\":SHV, \"OXA\":OXA, \"CMY\":CMY, \n",
    "            \"ACT\":ACT, \"LAT\":LAT}\n",
    "families_all = TEM + CTXM + SHV + OXA + CMY + ACT + LAT\n",
    "for i in blms:\n",
    "    assert i in families_all\n",
    "#Creating intersection lists\n",
    "combined = {}\n",
    "for i in list(families.keys()):\n",
    "    for j in list(bush_jacoby_classes.keys()):\n",
    "        x = sorted(list(set(bush_jacoby_classes[j])& set(families[i])))\n",
    "        if x != []:\n",
    "            combined[i+'_'+j] = x\n",
    "    combined[i + \"_OTHER\"] = []\n",
    "    \n",
    "# Cats with > 10 isolates\n",
    "individual_cats = ['TEM_buj2b', 'CTXM_buj2be', 'SHV_buj2b', 'OXA_buj2d']\n",
    "other_cats = [i for i in combined if i not in individual_cats]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now for each isolate we then approtion categories, \n",
    "\n",
    "This is done using the model_cat class. This basically reads everything in , then runs make_cat to actually perform the categorization. the make_output function then actually makes the output required for an intermediate file which contains model categories alone (not merged with MICs)\n",
    "\n",
    "\n",
    "**NOTE:**\n",
    "There are 2 important steps which deal with atypical cases to highlight\n",
    "1. One blaSHV in AMC_926 had its (outlier) copy number truncated to reduce its effect in the overall models\n",
    "2. 11 samples in the \"testing\" dataset have had some of their model components set to 0 as they were not seen in the training set, and hence these were assigned an effect of 0\n",
    "\n",
    "Both of these changes happen in the make_cat function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Just a useful function\n",
    "def convert_bool(s):\n",
    "    if s == False:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "# Now that we have set up a way to categorize things\n",
    "# Next step is to then code in actually categorizing each sample.\n",
    "\n",
    "class model_cat:\n",
    "    \n",
    "    def __init__(self, guuid):\n",
    "        self.guuid = guuid\n",
    "        self.mlst = mlsts.loc[self.guuid].mlst\n",
    "        self.mlst_cat = mlsts.loc[self.guuid].mlst_cat\n",
    "        self.mlst_131 = mlsts.loc[self.guuid].mlst_131\n",
    "        self.mlst_95 = mlsts.loc[self.guuid].mlst_95\n",
    "        self.mlst_73 = mlsts.loc[self.guuid].mlst_73\n",
    "        self.mlst_69 = mlsts.loc[self.guuid].mlst_69\n",
    "        self.complete_agardil = mlsts.loc[self.guuid].complete_agardil\n",
    "        self.exclusion = convert_bool(self.guuid in exclusions.index)\n",
    "        self.dict = model_start[self.guuid]\n",
    "        self.dict_rename = {k.split(\".\")[0].replace(\"TEM-1D\", 'TEM-1'):self.dict['blm'][k] for k in self.dict['blm'].keys()}\n",
    "        self.blms = [x.split(\".\")[0].replace(\"TEM-1D\", 'TEM-1') for x in self.dict['blm'].keys() if x.split(\".\")[0].replace(\"TEM-1D\", 'TEM-1') in blms]\n",
    "        self.agar_dil = (self.guuid in agar_guuids)\n",
    "        self.model_cats = self.make_cat()\n",
    "        \n",
    "    def make_cat(self):\n",
    "        # This is what actually makes components, and basically it does this by deciding what blm class the component is\n",
    "        # Note the actual categories represent the commonest elements seen in our dataset, as other categories each \n",
    "        # (e.g. IRT tem in the agar dilution dataset) are too rare to do anything other than put into an other category.\n",
    "        # For category components in agar dil data see final table\n",
    "        cats = {k:0 for k in combined.keys()}\n",
    "        cats_final = {k:0 for k in ['TEM_buj2b', 'CTXM_buj2be', 'SHV_buj2b', 'OXA_buj2d', 'OTHER_bla']}        \n",
    "        for k in combined.keys():\n",
    "            for b in self.blms:\n",
    "                if b in combined[k]:\n",
    "                    cats[k] += self.dict_rename[b]\n",
    "        for k in cats.keys():\n",
    "            if k in cats_final.keys():\n",
    "                cats_final[k] = cats[k]\n",
    "            else:\n",
    "                if cats[k] != 0:\n",
    "                    cats_final['OTHER_bla'] = 1\n",
    "        # So this categorises everything, however, in the \"test set\" there are several non-seen elements\n",
    "        # whose effects cannot be reliably estimated using the \"training set data\".\n",
    "        # These have been categorised here, however, given their effect are more conservatively described as 0 as\n",
    "        # They are unknown, they are set to zero.\n",
    "        # This is done on a per sample basis. (see exclusions doc for more detail)\n",
    "        if self.exclusion == 1:\n",
    "            cats_final[exclusions.loc[self.guuid].element] = 0\n",
    "        # In addition we truncate one outlier blaSHV copy number (see supplementary methods)\n",
    "        if self.guuid == \"AMC_926\":\n",
    "            cats_final['SHV_buj2b'] = 16.267611\n",
    "        return cats_final\n",
    "\n",
    "    def make_output(self):\n",
    "        # This then writes a sample specific line in the model categorization\n",
    "        output = {}\n",
    "        output['guuid'] = self.guuid\n",
    "        for i in self.model_cats.keys():\n",
    "            output[\"bla\" + i.lower()] = self.model_cats[i]\n",
    "        output['ampcprother_sigampcpr'] = convert_bool(self.dict['ampc'])\n",
    "        output['tempromter_sigtem'] = convert_bool(self.dict['temp'])\n",
    "        output['ompnon_functioning'] = convert_bool(self.dict['nfomp'])\n",
    "        output['inagar'] = self.agar_dil\n",
    "        output[\"exclusion\"] = self.exclusion\n",
    "        output['mlst'] = self.mlst\n",
    "        output['mlst_cat'] = self.mlst_cat\n",
    "        output[\"mlst_131\"] = self.mlst_131\n",
    "        output[\"mlst_95\"] = self.mlst_95\n",
    "        output[\"mlst_73\"] = self.mlst_73\n",
    "        output[\"mlst_69\"] = self.mlst_69\n",
    "        output[\"complete_agardil\"] = self.complete_agardil\n",
    "        return output\n",
    "        \n",
    "# Actually carrying out the categorization\n",
    "categories = {k:0 for k in combined.keys()}\n",
    "\n",
    "with open(\"model_categories.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f, delimiter = \",\")\n",
    "    writer.writerow(['guuid', 'blatem_buj2b', 'blactxm_buj2be', 'blashv_buj2b', 'blaoxa_buj2d', 'blaother_bla', 'ampcprother_sigampcpr', 'temprother_sigtem', 'ompnon_functioning', 'inagar', \"prediction_exclusion\", \"mlst\", \"mlst_cat\", \"mlst_131\", \"mlst_95\", \"mlst_73\", \"mlst_69\", \"complete_agardil\"])\n",
    "    for i in guuids:\n",
    "        x = model_cat(i)\n",
    "        writer.writerow([x.make_output()[i] for i in \n",
    "                         ['guuid', 'blatem_buj2b', 'blactxm_buj2be', 'blashv_buj2b', 'blaoxa_buj2d', 'blaother_bla', 'ampcprother_sigampcpr', 'tempromter_sigtem', 'ompnon_functioning', 'inagar', \"exclusion\", \"mlst\", \"mlst_cat\", \"mlst_131\", \"mlst_95\", \"mlst_73\", \"mlst_69\", \"complete_agardil\"]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next step is to merge these with the agar dilution MIC data\n",
    "\n",
    "\n",
    "This merges the intermediate file (model_categories.csv) with all the phenotype data, (bd phoenix and agar dilution) It then combines these and writes them out into the final model_base.csv for input to the prediction file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we merge this with the routine and agar dilution MIC data\n",
    "\n",
    "\n",
    "#Reading in data\n",
    "\n",
    "cat_df = pd.read_csv(\"model_categories.csv\", index_col = 0)\n",
    "mlsts = pd.read_csv(\"mlsts.csv\")\n",
    "guuids = list(cat_df.index)\n",
    "with open(\"agar_dilution_raw_mics.json\" , \"r\")  as f:\n",
    "    mics_df = json.load(f)\n",
    "routine_lab = pd.read_csv('bd_phoenix_mics.csv', index_col = 0)\n",
    "\n",
    "\n",
    "\n",
    "# Functions to convert MICs to human readable\n",
    "def convert_clsi_hrf(no):\n",
    "    no = no.lstrip(\"<=\")\n",
    "    no = no.lstrip(\">\")\n",
    "    no = int(float(no))\n",
    "    if no == 2.0:\n",
    "        start = \"<=\"\n",
    "    elif no == 256.0:\n",
    "        start = \">\"\n",
    "    else:\n",
    "        start = \"\"\n",
    "    x = start +  str(no) +\"/\" +str(int(no/2))\n",
    "    return x\n",
    "\n",
    "def convert_eucast_hrf(no):\n",
    "    no = no.lstrip(\"<=\")\n",
    "    no = no.lstrip(\">\")\n",
    "    no = int(float(no))\n",
    "    if no == 2.0:\n",
    "        start = \"<=\"\n",
    "    elif no == 256.0:\n",
    "        start = \">\"\n",
    "    else:\n",
    "        start = \"\"\n",
    "    x = start +  str(no) +\"/\" +str(2)\n",
    "    return x\n",
    "\n",
    "with open(\"model_base.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    header_line = ['guuid', 'blatem_buj2b', 'blactxm_buj2be', 'blashv_buj2b', 'blaoxa_buj2d', 'blaother_bla',\n",
    "                   'ampcprother_sigampcpr', 'temprother_sigtem', 'ompnon_functioning', 'inagar', \n",
    "                   'complete_agardil', 'mlst', 'mlst_cat', \"mlst_131\", \"mlst_95\",\"mlst_73\", \"mlst_69\", \n",
    "                   'coa_mic', \"prediction_exclusion\"]\n",
    "    header_line_1 = header_line + ['test_type', 'mic', 'untransformed_mic', 'batch_no']\n",
    "    writer.writerow(header_line_1)\n",
    "    for i in guuids:\n",
    "        i_out = {}\n",
    "        for k in cat_df.columns.values:\n",
    "            i_out[k] = cat_df.loc[i][k]\n",
    "        for k in routine_lab.columns.values:\n",
    "            i_out[k] = routine_lab.loc[i][k]\n",
    "        if i not in mics_df.keys(): # IE those NOT tested by agar dilution\n",
    "            j_list = []\n",
    "            for k in header_line:\n",
    "                if k == \"guuid\":\n",
    "                    j_list.append(i)\n",
    "                else:\n",
    "                    j_list.append(i_out[k])\n",
    "            # Next we add the test type , (which for all BD phoenix experiments is best approximated as EUCAST)  ie test type = 1\n",
    "            j_list.append(1) #test type , note we use this in the \"prediction step\"\n",
    "            j_list.append(\"\") # mic result -> NONE HERE\n",
    "            j_list.append(\"\") #untransformed mic\n",
    "            j_list.append(\"\") #batch no\n",
    "            writer.writerow(j_list)\n",
    "        else:\n",
    "            clsi_results = mics_df[i]['clsi']\n",
    "            for key in mics_df[i]['clsi'].keys():\n",
    "                batch_no = int(key.split(\"_\")[-1])\n",
    "                j_list = []\n",
    "                for k in header_line:\n",
    "                    if k == \"guuid\":\n",
    "                        j_list.append(i)\n",
    "                    else:\n",
    "                        j_list.append(i_out[k])\n",
    "                j_list.append(0)\n",
    "                # Note here, the MIC used in the model is actually the one below the limit of detection (i.e. 1.0  <=2/2) and the same aplies at the top end. (>128 becomes 256)\n",
    "                # See Croghan, C AND P P. Egeghy. METHODS OF DEALING WITH VALUES BELOW THE LIMIT OF DETECTION USING SAS. Presented at Southeastern SAS User Group, St. Petersburg, FL, September 22-24, 2003.\n",
    "                j_list.append(np.log2(mics_df[i]['clsi'][key]['MIC_FOR_MODEL_ACCOUNTING_FOR_CENSORING']))\n",
    "                j_list.append(convert_clsi_hrf(mics_df[i]['clsi'][key]['RECORDED_MIC']))\n",
    "                j_list.append(batch_no)\n",
    "                writer.writerow(j_list)\n",
    "            eucast_results = mics_df[i]['eucast']\n",
    "            for key in mics_df[i]['eucast'].keys():\n",
    "                batch_no = int(key.split(\"_\")[-1])\n",
    "                j_list = []\n",
    "                for k in header_line:\n",
    "                    if k == \"guuid\":\n",
    "                        j_list.append(i)\n",
    "                    else:\n",
    "                        j_list.append(i_out[k])\n",
    "                j_list.append(1)\n",
    "                # Note here, the MIC used in the model is actually the one below the limit of detection (i.e. 1.0  <=2/2) and the same aplies at the top end. (>128 becomes 256)\n",
    "                # See Croghan, C AND P P. Egeghy. METHODS OF DEALING WITH VALUES BELOW THE LIMIT OF DETECTION USING SAS. Presented at Southeastern SAS User Group, St. Petersburg, FL, September 22-24, 2003.\n",
    "                j_list.append(np.log2(mics_df[i]['eucast'][key]['MIC_FOR_MODEL_ACCOUNTING_FOR_CENSORING']))\n",
    "                j_list.append(convert_eucast_hrf(mics_df[i]['eucast'][key]['RECORDED_MIC']))\n",
    "                j_list.append(batch_no)\n",
    "                writer.writerow(j_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this process we have a file, model_base.csv, from which to build our mixed models\n",
    "**From here on in we switch to the STATA code for model fitting and predictions**\n",
    "\n",
    "### NOTE IN THE FINAL DATASET, COAMIC IS THE BD PHOENIX MIC, MIC is the AGAR DILUTION MIC. This is will corrected in the STATA do file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
